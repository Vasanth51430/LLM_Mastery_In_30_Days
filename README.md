# ğŸš€ LLM Mastery in 30 Days

Embark on a transformative, step-by-step journey through the world of Large Language Models (LLMs) with this immersive 30-day course. From foundational concepts to deploying real-world applications, this course is designed to empower you to build, train, and deploy your own LLMs, one day at a time.

---

# ğŸ“œ Course Weekly Links

| Week   | Contents  | Video Link | 
|--------|-----------|------------|
| Week 1 | Transformers Introduction, Types, Tokenizers Working, Transformers Indepth Math, Transformers Scratch Implementation, Architectural Advancements in Positional Encoding and Attention, LLM Pretraining Guide | [Video](https://youtu.be/xkv6GbMIe4g)

---

### **ğŸŒŸ What Youâ€™ll Learn:**

**ğŸ§  Transformers Uncovered:**
- ğŸ” **Foundational Concepts**: Explore the core ideas behind transformers, including attention mechanisms, tokenization, and various transformer types.
- ğŸ“œ **Paper & Theory**: Study the breakthrough "Attention is All You Need" paper and gain insights into how it revolutionized the field.
- ğŸ”§ **Hands-On Implementation**: Code a transformer from scratch in PyTorch, with deep dives into each part, from positional encoding to different attention mechanisms.

**ğŸ› ï¸ Building the Model - Architectural Mastery:**
- ğŸ¯ **Positional Encoding & Attention Mechanisms**: Understand each component of transformer architecture, experimenting with positional encodings and attention mechanisms.
- ğŸ” **Pretraining Essentials**: Gather and process data, create PyTorch datasets, and gain hands-on experience building a miniature version of Llama3 for a solid grasp of pretraining.

**ğŸš€ Advanced Fine-Tuning & Evaluation:**
- ğŸ’¡ **Fine-Tuning Techniques**: Master fine-tuning approaches, such as LoRA, SFT (Supervised Fine-Tuning), and Hugging Face workflows, to tailor models for specific tasks.
- ğŸ§ª **Evaluation Metrics**: Measure model performance with tools like perplexity, and fine-tune for specialized applications with metrics that ensure high-quality results.
- ğŸ”„ **Quantization & Optimization**: Streamline your model using techniques like quantization, pruning, and distillation for improved efficiency.

**ğŸ’¡ Real-World Applications & Deployment:**
- ğŸ› ï¸ **RAG & Agent Building**: Build a Retrieval-Augmented Generation (RAG) model from scratch and design simple agents capable of handling real-world NLP tasks.
- ğŸŒ **Deploying Your Model**: Use frameworks like Ollama and LitServe to serve your model live, making it accessible to users via a web-ready NLP application.
- ğŸ”„ **Model Merging & MOE**: Experiment with model merging techniques and Mixture of Experts (MoE) architectures to enhance model capabilities.

---

With **step-by-step guidance**, **daily coding exercises**, and **practical deployment** examples, **ğŸš€ 30 Days to LLM Mastery** will make you an LLM expert, ready to create, fine-tune, and deploy powerful language models. Join us and transform your skills in the world of LLMs!

---

# ğŸ“œ Contents Covered - Day Wise

Here are the contents which are covered day wise with the topics and links provided for each day

| Day   | Topic                                                           | Video Link                                                                 | Code Link                                                                                             |
|-------|-----------------------------------------------------------------|---------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|
| Day 0 | LLM Prerequisites - All You Need To Know To Start Your LLM Journey | [Video](https://www.youtube.com/watch?v=aZePVtBHYhM)                    | [Code](https://github.com/Vasanth51430/LLM_Mastery_In_30_Days/tree/main/Day0)                        |
| Day 1 | Introduction to Transformers, Transformers Types and its Working | [Video](https://www.youtube.com/watch?v=-4OMIgNmeDc)                    | [Code](https://github.com/Vasanth51430/LLM_Mastery_In_30_Days/tree/main/Day1)                        |
| Day 2 | How Tokenizers Work in LLMs?                                    | [Video](https://www.youtube.com/watch?v=mlq5VblXNqk)                    | [Code](https://github.com/Vasanth51430/LLM_Mastery_In_30_Days/tree/main/Day2)                        |
| Day 3 | The Math Behind Transformers                                    | [Video](https://www.youtube.com/watch?v=Xos-JTRMvfY)                    | [Code](https://github.com/Vasanth51430/LLM_Mastery_In_30_Days/tree/main/Day3)                        |
| Day 4 | Transformers Implemented From Scratch in Pytorch                | [Video](https://www.youtube.com/watch?v=lB8oxvuE1-Q)                    | [Code](https://github.com/Vasanth51430/LLM_Mastery_In_30_Days/tree/main/Day4)                        |
| Day 5 | LLM Architectural Advancements - Positional Encoding Mechanisms | [Video](https://www.youtube.com/watch?v=ktdVRiDkWM4)                    | [Code](https://github.com/Vasanth51430/LLM_Mastery_In_30_Days/tree/main/Day5)                        |
| Day 6 | LLM Architectural Advancements - Attention Mechanisms           | [Video](https://www.youtube.com/watch?v=k4FSaLTolME)                    | [Code](https://github.com/Vasanth51430/LLM_Mastery_In_30_Days/tree/main/Day6)                        |
| Day 7 | LLM Pretraining Course - Master LLM Pretraining in 1 Hour       | [Video](https://www.youtube.com/watch?v=21EejfdJYIU)                    | [Code](https://github.com/Vasanth51430/LLM_Mastery_In_30_Days/tree/main/Day7)                        |
| Day 8 | Build MiniGPT 2 From Scratch                                    | [Video](https://www.youtube.com/watch?v=H3PY4gUEkd8)                    | [Code](https://github.com/Vasanth51430/LLM_Mastery_In_30_Days/tree/main/Day8)                        |

---
